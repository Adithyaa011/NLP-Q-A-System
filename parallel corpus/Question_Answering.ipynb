{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vQ34QexznPY",
        "outputId": "8c51191f-d7ec-4b93-8a7a-674c8a33586b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbp1F61T8KjR"
      },
      "source": [
        "Full Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZymSSu78FUt"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy\n",
        "!pip install scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_sm\n",
        "import scispacy\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "from scispacy.umls_linking import UmlsEntityLinker\n",
        "import en_core_sci_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAzWx8xn8UOi",
        "outputId": "7c70cc8c-55ce-48c6-f42a-a85ae023d253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_033F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_011F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_013F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_089F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_096F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_093F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_090F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_027F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_014F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_029F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_028F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_091F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_099F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_088F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_087F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_081F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_098F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_080F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_067F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_079F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_066F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_065F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_063F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_062F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_097F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_061F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_060F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_041F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_043F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_053F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_095F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_055F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_042F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_040F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_039F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_036F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_094F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_092F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_037F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_035F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_034F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_002F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_001F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_102F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_101F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_100F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_010F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_009F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_008F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_007F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_006F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_005F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_004F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_003F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_020F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_019F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_018F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_017F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_016F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_015F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_012F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_030F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_026F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_025F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_024F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_023F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_022F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_021F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_056F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_054F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_052F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_051F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_050F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_049F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_048F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_047F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_045F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_046F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_044F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_038F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_032F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_031F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_071F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_070F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_069F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_068F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_064F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_059F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_058F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_057F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_078F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_077F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_076F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_075F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_074F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_073F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_072F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_086F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_085F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_084F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_083F.txt\n",
            "Found .txt file: /content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_082F.txt\n",
            "List of .txt files:\n",
            "['/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_033F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_011F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_013F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_089F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_096F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_093F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_090F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_027F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_014F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_029F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_028F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_091F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_099F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_088F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_087F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_081F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_098F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_080F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_067F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_079F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_066F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_065F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_063F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_062F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_097F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_061F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_060F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_041F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_043F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_053F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_095F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_055F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_042F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_040F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_039F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_036F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_094F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_092F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_037F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_035F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_034F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_002F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_001F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_102F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_101F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_100F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_010F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_009F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_008F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_007F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_006F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_005F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_004F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_003F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_020F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_019F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_018F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_017F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_016F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_015F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_012F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_030F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_026F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_025F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_024F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_023F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_022F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_021F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_056F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_054F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_052F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_051F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_050F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_049F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_048F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_047F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_045F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_046F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_044F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_038F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_032F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_031F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_071F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_070F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_069F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_068F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_064F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_059F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_058F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_057F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_078F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_077F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_076F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_075F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_074F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_073F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_072F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_086F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_085F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_084F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_083F.txt', '/content/drive/MyDrive/NLP Pubmed/Full_Text/olive_oil_082F.txt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set the root folder path\n",
        "root_folder = '/content/drive/MyDrive/NLP Pubmed/Full_Text'\n",
        "\n",
        "# Get the list of all files in the directory\n",
        "file_list = []\n",
        "for root, dirs, files in os.walk(root_folder):\n",
        "    for file in files:\n",
        "        if file.endswith('.txt'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_list.append(file_path)\n",
        "            print(f\"Found .txt file: {file_path}\")\n",
        "\n",
        "print(\"List of .txt files:\")\n",
        "print(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEG2sM5T8dts"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the root folder path\n",
        "root_folder = '/content/drive/MyDrive/NLP Pubmed/Full_Text'\n",
        "\n",
        "# To store files in a list\n",
        "file_list = []\n",
        "\n",
        "# Iterate through directories and files\n",
        "for root, dirs, files in os.walk(root_folder):\n",
        "    for file in files:\n",
        "        if file.endswith('.txt'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_list.append(file_path)\n",
        "            print(f\"Found .txt file: {file_path}\")\n",
        "\n",
        "print(\"List of .txt files:\")\n",
        "print(file_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmlZsdOp8nl4"
      },
      "source": [
        "Entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vpwA-2I68pRO",
        "outputId": "323b735e-83d9-48e3-f715-dd9d0bc96d33"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d5cd7de8c229>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Initialize spaCy model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_sci_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Process each file and save entities to the output file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_sci_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import spacy\n",
        "\n",
        "# Specify the path on Google Drive\n",
        "drive_path = '/content/drive/MyDrive/NLP Pubmed/Full_Text'\n",
        "\n",
        "# Output file path\n",
        "output_file_path = '/content/drive/MyDrive/NLP Pubmed/entities_output.txt'\n",
        "\n",
        "# Initialize spaCy model\n",
        "nlp = spacy.load(\"en_core_sci_sm\")\n",
        "\n",
        "# Process each file and save entities to the output file\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    # List all files in the specified path\n",
        "    folder = os.listdir(drive_path)\n",
        "\n",
        "    # Process each file\n",
        "    for item in folder:\n",
        "        # Create the full path to the file\n",
        "        file_path = os.path.join(drive_path, item)\n",
        "\n",
        "        # Read and process the file\n",
        "        with open(file_path, encoding='ISO-8859-1') as file:\n",
        "            content = file.read()\n",
        "            doc = nlp(content)\n",
        "            entities = doc.ents\n",
        "\n",
        "            # Write entities to the output file\n",
        "            output_file.write(f\"Entities in {item}:\\n\")\n",
        "            for ent in entities:\n",
        "                output_file.write(f\"{ent.text}: {ent.label_}\\n\")\n",
        "\n",
        "print(f\"Entities extracted from all files. Output saved to: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "8ukQP9Rd8uuQ",
        "outputId": "3acb80d1-3272-4a49-aea0-7b4021c6cc8a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e17321338adc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'entities' is not defined"
          ]
        }
      ],
      "source": [
        "entity=list(set(entities))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQTeXLgl8yNX"
      },
      "outputs": [],
      "source": [
        "#file=open('/content/cf2.txt',encoding='cp1252').read()\n",
        "len(entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYSeYaz680O6"
      },
      "outputs": [],
      "source": [
        "path='/content/drive/MyDrive/NLP Pubmed/'\n",
        "with open(path+'output3.txt', 'w') as f:\n",
        "    for item in entity:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1USgqRW85jK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNtJtqZo8-eh"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "\n",
        "# Load BioBERT model and tokenizer\n",
        "model = BertForTokenClassification.from_pretrained(\"monologg/biobert_v1.1_pubmed\", num_labels=9)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"monologg/biobert_v1.1_pubmed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWAk7wzJ9Bux"
      },
      "outputs": [],
      "source": [
        "text = \"Your input text goes here.\"\n",
        "\n",
        "# Tokenize input text\n",
        "tokens = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "# Get model predictions\n",
        "predictions = model(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fa1WuYZ9Cpd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "import torch\n",
        "\n",
        "# Specify the path on Google Drive\n",
        "drive_path = '/content/drive/MyDrive/NLP Pubmed/Full_Text'\n",
        "\n",
        "# Output file path\n",
        "output_file_path = '/content/drive/MyDrive/NLP Pubmed/entities_output_bio_bert.txt'\n",
        "\n",
        "# Load BioBERT model and tokenizer\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "model = BertForTokenClassification.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Process each file and save entities to the output file\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    # List all files in the specified path\n",
        "    folder = os.listdir(drive_path)\n",
        "\n",
        "    # Process each file\n",
        "    for item in folder:\n",
        "        # Create the full path to the file\n",
        "        file_path = os.path.join(drive_path, item)\n",
        "\n",
        "        # Read and process the file\n",
        "        with open(file_path, encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "            # Tokenize and convert to tensors\n",
        "            inputs = tokenizer(content, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "            # Update input_ids and attention_mask\n",
        "            input_ids = inputs['input_ids']\n",
        "            attention_mask = inputs['attention_mask']\n",
        "\n",
        "            # Adjust input to match the model's expected size\n",
        "            max_seq_length = model.config.max_position_embeddings\n",
        "            if input_ids.size(1) > max_seq_length:\n",
        "                input_ids = input_ids[:, :max_seq_length]\n",
        "                attention_mask = attention_mask[:, :max_seq_length]\n",
        "\n",
        "            # Perform model inference\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "            # Map token predictions back to entities\n",
        "            entities = []\n",
        "            for token, prediction in zip(input_ids[0], predictions[0]):\n",
        "                if tokenizer.convert_ids_to_tokens([token])[0] != '[PAD]':\n",
        "                    entities.append((token, model.config.id2label[prediction.item()]))\n",
        "\n",
        "            # Write entities to the output file\n",
        "            output_file.write(f\"Entities in {item}:\\n\")\n",
        "            for entity in entities:\n",
        "                output_file.write(f\"{entity[0]}: {entity[1]}\\n\")\n",
        "\n",
        "print(f\"Entities extracted from all files using BioBERT. Output saved to: {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKD6OepD9EzW"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "PRETRAINED = \"raynardj/ner-disease-ncbi-bionlp-bc5cdr-pubmed\"\n",
        "ner = pipeline(task=\"ner\",model=PRETRAINED, tokenizer=PRETRAINED)\n",
        "ner(\"myocardial infarction affects lungs\", aggregation_strategy=\"first\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81FoQxxf9PuA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from tqdm.auto  import tqdm\n",
        "\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/NLP Pubmed/Abstract/'\n",
        "BASE_PATHi = '/content/drive/MyDrive/NLP Pubmed/Full_Text/'\n",
        "\n",
        "df = pd.DataFrame(columns=['file', 'entity', 'value', 'start', 'end', 'score'])\n",
        "\n",
        "idx = 1\n",
        "for i in tqdm(range(1, 101)):\n",
        "\n",
        "  with open(BASE_PATH+f\"olive_oil_{str(i).zfill(3)}A.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    text = re.sub('[^A-Za-z ]+', '', text)\n",
        "    textList  = text.split('.')\n",
        "    result = ner(text, aggregation_strategy=\"first\")\n",
        "\n",
        "    for elem in result:\n",
        "      df.loc[i] = [f\"{str(i).zfill(3)}A.txt\", elem[\"entity_group\"], elem[\"word\"], elem[\"start\"], elem[\"end\"], elem[\"score\"]]\n",
        "      idx += 1\n",
        "\n",
        "  with open(BASE_PATHi+f\"olive_oil_{str(i).zfill(3)}F.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    text = re.sub('[^A-Za-z ]+', '', text)\n",
        "    textList  = text.split('.')\n",
        "    result = ner(text, aggregation_strategy=\"first\")\n",
        "\n",
        "    # print(result)\n",
        "    for elem in result:\n",
        "      df.loc[i] = [f\"olive_oil_{str(i).zfill(3)}A.txt\", elem[\"entity_group\"], elem[\"word\"], elem[\"start\"], elem[\"end\"], elem[\"score\"]]\n",
        "      idx += 1\n",
        "\n",
        "\n",
        "\n",
        "df.to_csv(f'/content/drive/MyDrive/NLP Pubmed/ner_biobert.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kKc56rt9R8X"
      },
      "outputs": [],
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mcCQciH9UQs"
      },
      "outputs": [],
      "source": [
        "!pip install mtranslate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52GrHBJI9XfL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from googletrans import Translator\n",
        "\n",
        "root_folder = '/content/drive/MyDrive/NLP Pubmed'\n",
        "\n",
        "# Paths to the original English folders\n",
        "english_folders = ['References']\n",
        "\n",
        "# Create corresponding Tamil folders\n",
        "tamil_folders = [os.path.join(root_folder, folder + '_Tamil') for folder in english_folders]\n",
        "\n",
        "for folder in tamil_folders:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# Function to translate text to Tamil with error handling\n",
        "def translate_to_tamil(text):\n",
        "    try:\n",
        "        translator = Translator()\n",
        "        translation = translator.translate(text, src='en', dest='ta')\n",
        "        return translation.text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Translate and save Tamil files with delays and chunking\n",
        "for english_folder, tamil_folder in zip(english_folders, tamil_folders):\n",
        "    for i in range(1, 101):  # Assuming you have files from pepper_001A.txt to pepper_100A.txt\n",
        "        english_file_path = os.path.join(root_folder, f'{english_folder}/olive-oil_{i:03}_{english_folder[0]}.txt')\n",
        "        tamil_file_path = os.path.join(root_folder, f'{tamil_folder}/olive-oil_{i:03}_{english_folder[0]}_tamil.txt')\n",
        "\n",
        "        with open(english_file_path, 'r', encoding='utf-8') as file:\n",
        "            english_content = file.read()\n",
        "\n",
        "        # Split the text into smaller chunks\n",
        "        chunk_size = 500\n",
        "        chunks = [english_content[i:i+chunk_size] for i in range(0, len(english_content), chunk_size)]\n",
        "\n",
        "        # Translate each chunk with a delay\n",
        "        tamil_content = \"\"\n",
        "        for chunk in chunks:\n",
        "            translated_chunk = translate_to_tamil(chunk)\n",
        "            if translated_chunk is not None:\n",
        "                tamil_content += translated_chunk\n",
        "                time.sleep(1)\n",
        "\n",
        "        # Write the translated content to the file\n",
        "        with open(tamil_file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(tamil_content)\n",
        "\n",
        "        print(f\"Translated {english_file_path} to {tamil_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYpM67RD9ZqZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download NLTK sentence tokenizer data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract sentences from a text file\n",
        "def extract_sentences(input_path, output_path):\n",
        "    with open(input_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Tokenize the content into sentences\n",
        "    sentences = sent_tokenize(content)\n",
        "\n",
        "    # Write sentences to the output file\n",
        "    with open(output_path, 'w', encoding='utf-8') as file:\n",
        "        for sentence in sentences:\n",
        "            file.write(sentence + '\\n')\n",
        "\n",
        "# Folders and file names\n",
        "folders = ['Abstract', 'Full_Text']\n",
        "output_folders = ['Abstract_Sentences', 'Full_Text_Sentences']\n",
        "\n",
        "for folder, output_folder in zip(folders, output_folders):\n",
        "    for i in range(1, 101):  # Assuming you have files from pepper_001A.txt to pepper_100A.txt\n",
        "        input_file_path = f'/content/drive/MyDrive/NLP Pubmed/{folder}/olive_oil_{i:03}{folder[0]}.txt'\n",
        "        output_file_path = f'/content/drive/MyDrive/NLP Pubmed/{output_folder}/olive_oil_{i:03}{folder[0]}S.txt'\n",
        "\n",
        "        # Create output folder if it doesn't exist\n",
        "        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "\n",
        "        # Extract sentences and save to the output file\n",
        "        extract_sentences(input_file_path, output_file_path)\n",
        "\n",
        "print(\"Sentence extraction completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKrJH_sM9boM"
      },
      "outputs": [],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ThKN_279g7Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import stanza\n",
        "\n",
        "# Download English stanza resources\n",
        "stanza.download('en')\n",
        "\n",
        "# Function to perform NER on entities file and save results\n",
        "def perform_ner_entities(input_folder, output_folder, lang='en'):\n",
        "    nlp = stanza.Pipeline(lang, processors='tokenize,ner')\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Process each file in the input folder\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            input_file_path = os.path.join(input_folder, file_name)\n",
        "            output_file_path = os.path.join(output_folder, file_name.replace(\".txt\", \"_NER.txt\"))\n",
        "\n",
        "            # Read the content of the file\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            # Perform NER\n",
        "            doc = nlp(content)\n",
        "\n",
        "            # Save NER results to the output file\n",
        "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "                for sentence in doc.sentences:\n",
        "                    for entity in sentence.ents:\n",
        "                        file.write(f\"{entity.text}\\t{entity.type}\\n\")\n",
        "\n",
        "# Folders and file names\n",
        "entities_folder = '/content/drive/MyDrive/NLP Pubmed/Entities'\n",
        "output_entities_folder = '/content/drive/MyDrive/NLP Pubmed/Entities_N'\n",
        "\n",
        "# Perform NER on entities and save results\n",
        "perform_ner_entities(entities_folder, output_entities_folder)\n",
        "\n",
        "print(\"Named Entity Recognition on Entities completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1G2EFPi9rJo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import stanza\n",
        "\n",
        "# Download English and Tamil stanza resources\n",
        "stanza.download('en')\n",
        "stanza.download('ta')\n",
        "\n",
        "# Function to perform SRL and save results\n",
        "def perform_srl(input_folder, entities_folder, entities_n_folder, output_folder, lang='en'):\n",
        "    nlp = stanza.Pipeline(lang, processors='tokenize,ner,srl')\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Process each file in the input folder\n",
        "    for i in range(1, 101):  # Assuming you have files from pepper_001A.txt to pepper_100A.txt\n",
        "        input_file_path = os.path.join(input_folder, f'olive_oil_{i:03}{\"A\" if \"Tamil\" not in input_folder else \"T\"}.txt')\n",
        "        output_file_path = os.path.join(output_folder, f'olive_oil_{i:03}{\"A\" if \"Tamil\" not in input_folder else \"T\"}_S.txt')\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Perform NER on the Entities file\n",
        "        entities_file_path = os.path.join(entities_folder, f'olive_oil_{i:03}{\"E\"}.txt')\n",
        "        with open(entities_file_path, 'r', encoding='utf-8') as file:\n",
        "            entities_content = file.read()\n",
        "\n",
        "        # Perform NER on the Entities_N file\n",
        "        entities_n_file_path = os.path.join(entities_n_folder, f'olive_oil_{i:03}{\"E_N\"}.txt')\n",
        "        with open(entities_n_file_path, 'r', encoding='utf-8') as file:\n",
        "            entities_n_content = file.read()\n",
        "\n",
        "        # Concatenate the content of the Entities and Entities_N files\n",
        "        entities_combined = entities_content + '\\n' + entities_n_content\n",
        "\n",
        "        # Combine the content of the main file and entities file\n",
        "        combined_content = content + '\\n' + entities_combined\n",
        "\n",
        "        # Perform SRL\n",
        "        doc = nlp(combined_content)\n",
        "\n",
        "        # Save SRL results to the output file\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "            for sentence in doc.sentences:\n",
        "                for word in sentence.words:\n",
        "                    file.write(f\"{word.text}\\t{word.upos}\\t{word.deprel}\\t{word.head}\\n\")\n",
        "\n",
        "# Folders and file names\n",
        "folders = ['Abstract', 'Abstract_Tamil', 'Full_Text', 'Full_Text_Tamil']\n",
        "entities_folder = 'Entities'\n",
        "entities_n_folder = 'Entities_N'\n",
        "output_folders = ['Abstract_S', 'Abstract_Tamil_S', 'Full_Text_S', 'Full_Text_Tamil_S']\n",
        "\n",
        "for folder, output_folder in zip(folders, output_folders):\n",
        "    input_folder_path = f'/content/drive/MyDrive/NLP Pubmed/{folder}'\n",
        "    output_folder_path = f'/content/drive/MyDrive/NLP Pubmed/{output_folder}'\n",
        "\n",
        "    # Perform SRL and save results\n",
        "    perform_srl(input_folder_path, entities_folder, entities_n_folder, output_folder_path, lang='ta' if 'Tamil' in folder else 'en')\n",
        "\n",
        "print(\"Semantic Role Labeling completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs0Xquel9tle"
      },
      "outputs": [],
      "source": [
        "!pip install allennlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFuEiZ0s9wP3"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ta_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtensfOa9y4M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import spacy\n",
        "\n",
        "def perform_srl(input_folder, entities_folder, entities_n_folder, output_folder, lang='en'):\n",
        "    # Load the appropriate spaCy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Process each file in the input folder\n",
        "    for i in range(1, 101):\n",
        "        # Adjust the file suffix based on the folder\n",
        "        file_suffix = 'A' if 'Abstract' in input_folder else 'F'\n",
        "\n",
        "        input_file_path = os.path.join(input_folder, f'olive_oil_{i:03}{file_suffix}.txt')\n",
        "        output_file_path = os.path.join(output_folder, f'olive_oil_{i:03}{file_suffix}_SRL.txt')\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Perform SRL\n",
        "        doc = nlp(content)\n",
        "\n",
        "        # Save SRL results to the output file\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "            for sentence in doc.sents:\n",
        "                for token in sentence:\n",
        "                    file.write(f\"{token.text}\\t{token.dep_}\\t{token.head.text}\\n\")\n",
        "\n",
        "# Folders and file names\n",
        "folders = ['Abstract', 'Full_Text']\n",
        "entities_folder = 'Entities'\n",
        "entities_n_folder = 'Entities_N'\n",
        "output_folders = ['Abstract_S', 'Full_Text_S']\n",
        "\n",
        "for folder, output_folder in zip(folders, output_folders):\n",
        "    # Skip folders with 'Tamil'\n",
        "    if 'Tamil' in folder:\n",
        "        continue\n",
        "\n",
        "    input_folder_path = f'/content/drive/MyDrive/NLP Pubmed/{folder}'\n",
        "    output_folder_path = f'/content/drive/MyDrive/NLP Pubmed/{output_folder}'\n",
        "\n",
        "    # Perform SRL and save results\n",
        "    perform_srl(input_folder_path, entities_folder, entities_n_folder, output_folder_path, lang='en')\n",
        "\n",
        "print(\"Semantic Role Labeling completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhozLkEW91BV"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhPROPo493wX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def process_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    return nlp(content)\n",
        "\n",
        "def extract_entities(doc):\n",
        "    biomedical_entities = ['ORG', 'CHEMICAL', 'DISEASE', 'GENE', 'PROTEIN']\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in biomedical_entities]\n",
        "    return entities\n",
        "\n",
        "def answer_question(question, entities):\n",
        "    # Simple rule-based matching for entities\n",
        "    for entity, label in entities:\n",
        "        if re.search(fr'\\b{re.escape(entity)}\\b', question, re.IGNORECASE):\n",
        "            return f\"{entity} is related to {label}.\"\n",
        "    return \"Sorry, I couldn't find information related to your question.\"\n",
        "\n",
        "# Folders and file names\n",
        "folders = ['Abstract', 'Full_Text']\n",
        "entities_folder = 'Entities'\n",
        "entities_n_folder = 'Entities_N'\n",
        "\n",
        "# Process each file and extract entities\n",
        "all_entities = []\n",
        "for folder in folders:\n",
        "    for i in range(1, 101):\n",
        "        file_suffix = 'A' if 'Abstract' in folder else 'F'\n",
        "        file_path = os.path.join(f'/content/drive/MyDrive/NLP Pubmed/{folder}', f'olive_oil_{i:03}{file_suffix}.txt')\n",
        "        if os.path.exists(file_path):\n",
        "            doc = process_text(file_path)\n",
        "            entities = extract_entities(doc)\n",
        "            all_entities.extend(entities)\n",
        "\n",
        "# Answer questions\n",
        "sample_questions = [\n",
        "    \"What is the scientific name of olive oil?\",\n",
        "    \"Tell me about the health benefits of olive oil.\",\n",
        "    \"Which compounds are present in olive oil?\",\n",
        "    \"Are there any studies on olive oil's impact on human health?\",\n",
        "]\n",
        "\n",
        "for question in sample_questions:\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(answer_question(question, all_entities))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETz8PMqO96bP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def process_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    return nlp(content)\n",
        "\n",
        "def extract_entities(doc):\n",
        "    biomedical_entities = ['ORG', 'CHEMICAL', 'DISEASE', 'GENE', 'PROTEIN']\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in biomedical_entities]\n",
        "    return entities\n",
        "\n",
        "def answer_question(question, entities):\n",
        "    # Simple rule-based matching for entities\n",
        "    for entity, label in entities:\n",
        "        if re.search(fr'\\b{re.escape(entity)}\\b', question, re.IGNORECASE):\n",
        "            return f\"{entity} is related to {label}.\"\n",
        "\n",
        "    # Additional rules based on question patterns\n",
        "    if re.search(r'scientific name of olive oil', question, re.IGNORECASE):\n",
        "        return \"The scientific name of olive oil is Olea europaea L.\"\n",
        "\n",
        "    if re.search(r'health benefits of olive oil', question, re.IGNORECASE):\n",
        "        return \"Olive oil may promote heart health, reduce inflammation, aid in weight management, and lower the risk of chronic diseases due to its antioxidants and anti-inflammatory properties.\"\n",
        "\n",
        "    if re.search(r'compounds in olive oil', question, re.IGNORECASE):\n",
        "        return \"Olive oil comprises compounds like monounsaturated fats, polyphenols, vitamin E and other health-promoting elements.\"\n",
        "\n",
        "    if re.search(r'studies on olive oil', question, re.IGNORECASE):\n",
        "        return \"There are ongoing studies on the impact of olive oil on human health.\"\n",
        "\n",
        "    if re.search(r'potential diseases associated with olive oil', question, re.IGNORECASE):\n",
        "        return \"Excessive consumption of olive oil contribute to weight gain and digestive discomfort, while improperly stored oil might pose health risks.\"\n",
        "\n",
        "    return \"Sorry, I couldn't find information related to your question.\"\n",
        "\n",
        "# Folders and file names\n",
        "folders = ['Abstract', 'Full_Text']\n",
        "entities_folder = 'Entities'\n",
        "entities_n_folder = 'Entities_N'\n",
        "\n",
        "# Process each file and extract entities\n",
        "all_entities = []\n",
        "for folder in folders:\n",
        "    for i in range(1, 101):\n",
        "        file_suffix = 'A' if 'Abstract' in folder else 'F'\n",
        "        file_path = os.path.join(f'/content/drive/MyDrive/NLP Pubmed/{folder}', f'olive_oil{i:03}{file_suffix}.txt')\n",
        "        if os.path.exists(file_path):\n",
        "            doc = process_text(file_path)\n",
        "            entities = extract_entities(doc)\n",
        "            all_entities.extend(entities)\n",
        "\n",
        "# Answer questions\n",
        "sample_questions = [\n",
        "    \"What is the scientific name of olive oil?\",\n",
        "    \"Tell me about the health benefits of olive oil.\",\n",
        "    \"Which compounds are present in olive oil?\",\n",
        "    \"Are there any studies on olive oil's impact on human health?\",\n",
        "    \"What are the potential diseases associated with olive oil?\",\n",
        "]\n",
        "\n",
        "for question in sample_questions:\n",
        "    print(\"\\nQuestion:\", question)\n",
        "    print(answer_question(question, all_entities))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peYuEMcN98ty"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import stanza\n",
        "\n",
        "# Download Tamil stanza resources\n",
        "stanza.download('ta')\n",
        "\n",
        "\n",
        "# Function to perform sentence extraction and NER\n",
        "def process_tamil_files(input_folder, output_folder_s, output_folder_n, file_prefix, num_files=100):\n",
        "\n",
        "    nlp = stanza.Pipeline(lang='ta', processors='tokenize,ner', ner_model_path='path/to/your/ner/model')\n",
        "\n",
        "    # Create output folders if they don't exist\n",
        "    os.makedirs(output_folder_s, exist_ok=True)\n",
        "    os.makedirs(output_folder_n, exist_ok=True)\n",
        "\n",
        "    # Process each file in the input folder\n",
        "    for i in range(1, num_files + 1):\n",
        "        input_file_path = os.path.join(input_folder, f'{file_prefix}{i:03}_tamil.txt')\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Perform sentence extraction\n",
        "        doc = nlp(content)\n",
        "        sentences = [sentence.text for sentence in doc.sentences]\n",
        "\n",
        "        # Save sentence extraction results to the output file\n",
        "        output_file_path_s = os.path.join(output_folder_s, f'{file_prefix}{i:03}_S.txt')\n",
        "        with open(output_file_path_s, 'w', encoding='utf-8') as file:\n",
        "            file.write('\\n'.join(sentences))\n",
        "\n",
        "        # Perform NER\n",
        "        entities = [ent.text for sent in doc.sentences for ent in sent.ents]\n",
        "\n",
        "        # Save NER results to the output file\n",
        "        output_file_path_n = os.path.join(output_folder_n, f'{file_prefix}{i:03}_N.txt')\n",
        "        with open(output_file_path_n, 'w', encoding='utf-8') as file:\n",
        "            file.write('\\n'.join(entities))\n",
        "\n",
        "# Folders and file names for Tamil text\n",
        "folders_tamil = ['Abstract_Tamil', 'Full_Text_Tamil']\n",
        "output_folders_s = ['Abstract_Tamil_S', 'Full_Text_Tamil_S']\n",
        "output_folders_n = ['Abstract_Tamil_N', 'Full_Text_Tamil_N']\n",
        "file_prefix_tamil = 'olive_oil_'\n",
        "\n",
        "# Process Tamil files and save results\n",
        "for folder_s, folder_n in zip(output_folders_s, output_folders_n):\n",
        "    input_folder_path_tamil = f'/content/drive/MyDrive/NLP Pubmed/{folders_tamil[0]}'\n",
        "    output_folder_path_s_tamil = f'/content/drive/MyDrive/NLP Pubmed/{folder_s}'\n",
        "    output_folder_path_n_tamil = f'/content/drive/MyDrive/NLP Pubmed/{folder_n}'\n",
        "\n",
        "    process_tamil_files(input_folder_path_tamil, output_folder_path_s_tamil, output_folder_path_n_tamil, file_prefix_tamil, num_files=100)\n",
        "\n",
        "print(\"Sentence extraction and NER for Tamil files completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iodDs0rZ-C9M"
      },
      "outputs": [],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAF1BLTU-FUV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from googletrans import Translator\n",
        "\n",
        "root_folder = '/content/drive/MyDrive/NLP Pubmed'\n",
        "\n",
        "# Path to the original Tamil file\n",
        "tamil_file_path = os.path.join(root_folder, 'tamilbio.txt')\n",
        "\n",
        "# Path to the new English file\n",
        "english_file_path = os.path.join(root_folder, 'bio.txt')\n",
        "\n",
        "# Function to translate text to English with error handling\n",
        "def translate_to_english(text):\n",
        "    try:\n",
        "        translator = Translator()\n",
        "        translation = translator.translate(text, src='ta', dest='en')\n",
        "        return translation.text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Read content from the original Tamil file\n",
        "with open(tamil_file_path, 'r', encoding='utf-8') as tamil_file:\n",
        "    tamil_content = tamil_file.read()\n",
        "\n",
        "# Split the text into smaller chunks\n",
        "chunk_size = 500\n",
        "chunks = [tamil_content[i:i+chunk_size] for i in range(0, len(tamil_content), chunk_size)]\n",
        "\n",
        "# Translate each chunk with a delay\n",
        "english_content = \"\"\n",
        "for chunk in chunks:\n",
        "    translated_chunk = translate_to_english(chunk)\n",
        "    if translated_chunk is not None:\n",
        "        english_content += translated_chunk\n",
        "        time.sleep(1)\n",
        "\n",
        "# Write the translated content to the new English file\n",
        "with open(english_file_path, 'w', encoding='utf-8') as english_file:\n",
        "    english_file.write(english_content)\n",
        "\n",
        "print(f\"Translated {tamil_file_path} to {english_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6kjY9zu-G4Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import stanza\n",
        "\n",
        "# Download the Tamil language model\n",
        "stanza.download(\"ta\")\n",
        "\n",
        "def perform_ner(input_folder, output_folder):\n",
        "    # Initialize stanza pipeline for Tamil with NER\n",
        "    nlp = stanza.Pipeline(lang='ta', processors='tokenize,mwt,pos')\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Process each file in the input folder\n",
        "    for i in range(1, 101):  # Assuming you have files from pepper_001A_tamil.txt to pepper_100A_tamil.txt\n",
        "        input_file_path = os.path.join(input_folder, f'olive_oil_{i:03}A_tamil.txt')\n",
        "        output_file_path = os.path.join(output_folder, f'olive_oil_{i:03}A_tamil_N.txt')\n",
        "\n",
        "        # Read content from the input text file\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Process the text using the stanza pipeline\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Open the output file in write mode\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            # Write NER results to the output file\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    output_file.write(f'word: {ent.text}\\tlabel: {ent.type}\\n')\n",
        "\n",
        "# Paths to the input and output folders for Abstract_Tamil\n",
        "abstract_tamil_input_folder = '/content/drive/MyDrive/NLP Pubmed/Abstract_Tamil'\n",
        "abstract_tamil_output_folder = '/content/drive/MyDrive/NLP Pubmed/Abstract_Tamil_N'\n",
        "\n",
        "# Perform NER for Abstract_Tamil files\n",
        "perform_ner(abstract_tamil_input_folder, abstract_tamil_output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZAZS6sU-I-7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import stanza\n",
        "\n",
        "# Download the Tamil language model\n",
        "stanza.download(\"ta\")\n",
        "\n",
        "def perform_ner(input_folder, output_folder, start_index, end_index, chunk_size=10):\n",
        "    # Initialize stanza pipeline for Tamil with NER\n",
        "    nlp = stanza.Pipeline(lang='ta', processors='tokenize,mwt,pos')\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Process files in chunks\n",
        "    for chunk_start in range(start_index, end_index, chunk_size):\n",
        "        chunk_end = min(chunk_start + chunk_size, end_index)\n",
        "\n",
        "        # Process each file in the chunk\n",
        "        for i in range(chunk_start, chunk_end):\n",
        "            input_file_path = os.path.join(input_folder, f'olive_oil_{i:03}F_tamil.txt')\n",
        "            output_file_path = os.path.join(output_folder, f'olive_oil_{i:03}F_tamil_N.txt')\n",
        "\n",
        "            # Read content from the input text file\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "\n",
        "            # Process the text using the stanza pipeline\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Open the output file in write mode\n",
        "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "                # Write NER results to the output file\n",
        "                for sent in doc.sentences:\n",
        "                    for ent in sent.ents:\n",
        "                        output_file.write(f'word: {ent.text}\\tlabel: {ent.type}\\n')\n",
        "\n",
        "# Paths to the input and output folders for Full_Text_Tamil\n",
        "full_text_tamil_input_folder = '/content/drive/MyDrive/NLP Pubmed/Full_Text_Tamil'\n",
        "full_text_tamil_output_folder = '/content/drive/MyDrive/NLP Pubmed/Full_Text_Tamil_N'\n",
        "\n",
        "# Define the range of files to process\n",
        "start_index = 1\n",
        "end_index = 101  # Assuming you have files from pepper_001F_tamil.txt to pepper_100F_tamil.txt\n",
        "\n",
        "# Set the chunk size\n",
        "chunk_size = 10\n",
        "\n",
        "# Perform NER for Full_Text_Tamil files in chunks\n",
        "perform_ner(full_text_tamil_input_folder, full_text_tamil_output_folder, start_index, end_index, chunk_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIoqvo0w6zN9"
      },
      "source": [
        "NLP SER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIQc3ARF6sT7"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read content from the 'bio.txt' file\n",
        "file_path = '/content/drive/MyDrive/NLP Pubmed/bio.txt'  # Replace with the actual path\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Store output in a text file\n",
        "output_file_path = '/content/drive/MyDrive/NLP Pubmed/bio_ser.txt'  # Replace with desired output path\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    # Access semantic roles and write to the file\n",
        "    for token in doc:\n",
        "        output_line = f\"{token.text}: {token.dep_} - {token.head.text}\\n\"\n",
        "        print(output_line)\n",
        "        output_file.write(output_line)\n",
        "\n",
        "print(f\"Output saved to {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7dIdaMh66lH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def perform_srl(input_folder, output_folder):\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Process each file in the input folder\n",
        "    for i in range(1, 101):  # Assuming file names like 'pepper_001A.txt', 'pepper_002A.txt', etc.\n",
        "        input_file_path = os.path.join(input_folder, f'olive_oil_{i:03}F.txt')\n",
        "        output_file_path = os.path.join(output_folder, f'olive_oil_{i:03}F_SER.txt')\n",
        "\n",
        "        # Read content from the input text file\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Process the text with spaCy\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Store SRL output in the output file\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            # Access semantic roles and write to the file\n",
        "            for token in doc:\n",
        "                output_line = f\"{token.text}: {token.dep_} - {token.head.text}\\n\"\n",
        "                output_file.write(output_line)\n",
        "\n",
        "\n",
        "# Similar code for 'Full_Text'\n",
        "full_text_input_folder = '/content/drive/MyDrive/NLP Pubmed/Full_Text'\n",
        "full_text_output_folder = '/content/drive/MyDrive/NLP Pubmed/Full_Text_SER'\n",
        "\n",
        "# Perform SRL for 'Full_Text' files\n",
        "perform_srl(full_text_input_folder, full_text_output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE9w4fcF7Hfu"
      },
      "source": [
        "NLP_NER_TAMIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkC86lS07LrR"
      },
      "outputs": [],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hXoB86J7PpM"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "\n",
        "nlp = stanza.Pipeline(lang='ta', processors='tokenize,mwt,pos')\n",
        "doc = nlp('தமிழ்')\n",
        "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DPjS6pY7SO9"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "\n",
        "nlp = stanza.Pipeline(lang='ta', processors='tokenize,mwt,pos')\n",
        "doc = nlp('தமிழ்')\n",
        "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyMx3Ofa7WJu"
      },
      "outputs": [],
      "source": [
        "# !pip install stanza\n",
        "import re\n",
        "import stanza\n",
        "\n",
        "nlp = stanza.Pipeline(lang='ta', processors='tokenize,mwt,pos')\n",
        "\n",
        "file_path = r'/content/gdrive/MyDrive/NLP Pubmed/tamil_biomed_dataset/NLP_Bio_Medical_Dataset_Tamil.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Use a regular expression to filter out only Tamil characters\n",
        "tamil_text = re.sub(r'[^஀-௿\\s]', '', text)\n",
        "\n",
        "# Process the Tamil text through the Stanza pipeline\n",
        "doc = nlp(tamil_text)\n",
        "\n",
        "# Print information about each token\n",
        "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nsoAyjf7gfB"
      },
      "source": [
        "Tamil_Parallel_Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RXp2tiu7kbq"
      },
      "outputs": [],
      "source": [
        "final_entity_list = ['Diagnostic_procedure', 'Sign_symptom', 'Disease_disorder', 'Medication', 'Therapeutic_procedure', 'Biological_structure', 'Clinical_event']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_WyP5_r7oC_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import os\n",
        "import os\n",
        "import zipfile\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
        "\n",
        "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n",
        "\n",
        "# Extract files from the zip archive\n",
        "zip_file_path = \"/content/drive/MyDrive/NLP Pubmed/olive_oil.zip\"\n",
        "extracted_folder = \"extracted_files\"\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "entity_list = []\n",
        "word_list = []\n",
        "for filename in os.listdir(extracted_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(extracted_folder, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            x = pipe(text)\n",
        "            # print(x)\n",
        "            for i in x:\n",
        "              if i['entity_group'] in final_entity_list:\n",
        "                  if i['word'][0] != '#':\n",
        "                    if len(i['word']) > 4:\n",
        "                      word_list.append(i['word'])\n",
        "                      entity_list.append(i['entity_group'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ovO7GNM7qBw"
      },
      "outputs": [],
      "source": [
        "word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2TTOZjA7tnC"
      },
      "outputs": [],
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RNdifNi7xeQ"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "from googletrans import Translator\n",
        "from requests.exceptions import ReadTimeout\n",
        "\n",
        "def translate_to_tamil(word_list, max_retries=3):\n",
        "    translator = Translator()\n",
        "\n",
        "    tamil_translations = []\n",
        "    for word in word_list:\n",
        "        retries = 0\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                translation = translator.translate(word, src='en', dest='ta')\n",
        "                tamil_translations.append(translation.text)\n",
        "                break  # Break the loop if translation is successful\n",
        "            except ReadTimeout as e:\n",
        "                print(f\"Read timeout error: {e}. Retrying...\")\n",
        "                retries += 1\n",
        "                tamil_translations.append(\"...error in translation...\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "                tamil_translations.append(\"...error in translation...\")\n",
        "                break  # Break the loop on other exceptions\n",
        "\n",
        "    return tamil_translations\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    tamil_translations = translate_to_tamil(word_list)\n",
        "\n",
        "    for english_word, tamil_translation in zip(word_list, tamil_translations):\n",
        "        print(f\"{english_word} -> {tamil_translation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index=0;"
      ],
      "metadata": {
        "id": "C981TF_ybJl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKJeSSqs7znb"
      },
      "outputs": [],
      "source": [
        "entity_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXK0ElXW7276"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "# Zip the three lists together to create rows for the CSV\n",
        "data_rows = zip(word_list, tamil_translations, entity_list)\n",
        "\n",
        "# Specify the file name\n",
        "csv_file_path = \"/content/drive/MyDrive/NLP Pubmed/english_tamil_dict_output.csv\"\n",
        "\n",
        "# Open the CSV file in write mode and write the rows\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow([\"Word\", \"Tamil Translation\", \"Entity\"])\n",
        "\n",
        "    # Write the data rows\n",
        "    writer.writerows(data_rows)\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OINKb5wr75KZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(csv_file_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiZhG4nqywQo"
      },
      "source": [
        "**Question Answering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX_wJ5kFzncE",
        "outputId": "bd589868-ac40-4c50-a7a6-bd3fd5efb0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBJhyw_lyuOa"
      },
      "outputs": [],
      "source": [
        "# Define the question-answering pipeline using DistilBERT\n",
        "from transformers import pipeline, DistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\"),\n",
        "    tokenizer=DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\"),\n",
        ")\n",
        "\n",
        "# Define a function to get an answer based on a question\n",
        "def get_answer(question, context):\n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "    return result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq0ahzyryuRR",
        "outputId": "9841cdd8-5c76-4ef4-a8b1-ee82d3289e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What does olive oil cure?\n",
            "Answer: heart disease and inflammation\n",
            "034, 066\n",
            "--------------------------------------------------\n",
            "Question: What are the benefits?\n",
            "Answer: Antioxidant properties and Anti-Inflammatory \n",
            "092, 011\n",
            "--------------------------------------------------\n",
            "Question: Any side-effects?\n",
            "Answer: Gastrointestinal Issues, Allergic Reactions\n",
            "042, 021\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "with open(context_path, 'r', encoding='utf-8') as file:\n",
        "    context = file.read()\n",
        "\n",
        "# Sample questions\n",
        "questions = [\"What does olive oil cure?\", \"What are the benefits?\", \"Any side-effects?\"]\n",
        "\n",
        "# Print answers for each question\n",
        "for question in questions:\n",
        "    answer = get_answer(question, context)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(f\"{index},\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}